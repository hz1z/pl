{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9dXpw3cGXVXBCw5+5w0iN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hz1z/pl/blob/main/pl_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mxECDDf8jxcl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.fc2 = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6EPQxhUNj3qr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 设置device"
      ],
      "metadata": {
        "id": "kn3vR6UCkEjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Et4h7CfKj-mO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 超参数"
      ],
      "metadata": {
        "id": "-t0PxImxkPfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "l_qw4uYikMce"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 设置数据"
      ],
      "metadata": {
        "id": "jWYlnk3MkUC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entire_dataset = datasets.MNIST(\n",
        "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "train_ds, val_ds = random_split(entire_dataset, [50000, 10000])\n",
        "test_ds = datasets.MNIST(\n",
        "    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGVZ4qfOkTX5",
        "outputId": "63b2fdce-54dc-4bd9-f110-a254a44cfd71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 114344325.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 38104968.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 88683465.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8017899.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 设置网络、损失、优化器"
      ],
      "metadata": {
        "id": "eiVYGoqEkpEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "M7g6syyZkxHg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 开始训练"
      ],
      "metadata": {
        "id": "tpoIsuAGk3s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # b,c,h,w ---> b,-1\n",
        "        data = data.reshape(data.shape[0], -1)\n",
        "\n",
        "        logits = model(data)\n",
        "        loss = criterion(logits, targets) #所有样本的均值\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient descent or adam step\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTSyBPs_k646",
        "outputId": "0abbfdcf-4ce5-48c5-8f09-956d3fa2f7c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:11<00:00, 67.95it/s]\n",
            "100%|██████████| 782/782 [00:15<00:00, 49.11it/s]\n",
            "100%|██████████| 782/782 [00:13<00:00, 60.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
        "    with torch.no_grad():\n",
        "        # Loop through the data\n",
        "        for x, y in loader:\n",
        "\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            # Get to correct shape\n",
        "            x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(x)\n",
        "            _, predictions = logits.max(1)\n",
        "\n",
        "            # Check how many we got correct\n",
        "            num_correct += (predictions == y).sum()\n",
        "\n",
        "            # Keep track of number of samples\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct / num_samples\n"
      ],
      "metadata": {
        "id": "rFgBgCzMliZ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check accuracy on training & test to see how good our model\n",
        "model.to(device)\n",
        "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
        "print(f\"Accuracy on validation set: {check_accuracy(val_loader, model)*100:.2f}\")\n",
        "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EerSbLZ7lnnk",
        "outputId": "a15b09af-d517-4825-b145-bdad1ac87191"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 95.77\n",
            "Accuracy on validation set: 95.77\n",
            "Accuracy on test set: 95.42\n"
          ]
        }
      ]
    }
  ]
}